{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXWZxLXa5OgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U keras\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z8D6rRA4WuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install Pillow\n",
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCbDD_rY_-7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwjn8DusKbw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import Input\n",
        "from keras.applications import VGG19\n",
        "#from keras.callbacks import TensorBoard\n",
        "from keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense\n",
        "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
        "#from tensorflow.keras.backend import set_session\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh1R-TZAKYqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U566PSdtebcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from scipy.misc import imread, imresize\n",
        "import cv2\n",
        "from math import log10, sqrt \n",
        "import cv2 \n",
        "import numpy as np \n",
        "\n",
        "def residual_block(x):\n",
        "\n",
        "    filters = [96, 96]\n",
        "    kernel_size = 3\n",
        "    strides = 1\n",
        "    padding = \"same\"\n",
        "    momentum = 0.8\n",
        "    activation = \"relu\"\n",
        "\n",
        "    res = Conv2D(filters=filters[0], kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
        "    res = Activation(activation=activation)(res)\n",
        "    res = BatchNormalization(momentum=momentum)(res)\n",
        "\n",
        "    res = Conv2D(filters=filters[1], kernel_size=kernel_size, strides=strides, padding=padding)(res)\n",
        "    res = BatchNormalization(momentum=momentum)(res)\n",
        "\n",
        "    # Add res and x\n",
        "    res = Add()([res, x])\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_generator():\n",
        "    \n",
        "    residual_blocks = 16\n",
        "    momentum = 0.8\n",
        "    input_shape = (96, 96, 3)\n",
        "\n",
        "    # Input Layer of the generator network\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Add the pre-residual block\n",
        "    gen1 = Conv2D(filters=96, kernel_size=9, strides=1, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # Add 16 residual blocks\n",
        "    res = residual_block(gen1)\n",
        "    for i in range(residual_blocks - 1):\n",
        "        res = residual_block(res)\n",
        "\n",
        "    # Add the post-residual block\n",
        "    gen2 = Conv2D(filters=96, kernel_size=3, strides=1, padding='same')(res)\n",
        "    gen2 = BatchNormalization(momentum=momentum)(gen2)\n",
        "\n",
        "    # Take the sum of the output from the pre-residual block(gen1) and the post-residual block(gen2)\n",
        "    gen3 = Add()([gen2, gen1])\n",
        "\n",
        "    # Add an upsampling block\n",
        "    gen4 = UpSampling2D(size=2)(gen3)\n",
        "    gen4 = Conv2D(filters=384, kernel_size=3, strides=1, padding='same')(gen4)\n",
        "    gen4 = Activation('relu')(gen4)\n",
        "\n",
        "    # Add another upsampling block\n",
        "    gen5 = UpSampling2D(size=2)(gen4)\n",
        "    gen5 = Conv2D(filters=384, kernel_size=3, strides=1, padding='same')(gen5)\n",
        "    gen5 = Activation('relu')(gen5)\n",
        "\n",
        "    # Output convolution layer\n",
        "    gen6 = Conv2D(filters=3, kernel_size=9, strides=1, padding='same')(gen5)\n",
        "    output = Activation('tanh')(gen6)\n",
        "\n",
        "    # Keras model\n",
        "    model = Model(inputs=[input_layer], outputs=[output], name='generator')\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_discriminator():\n",
        "    \"\"\"\n",
        "    Create a discriminator network using the hyperparameter values defined below\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    leakyrelu_alpha = 0.2\n",
        "    momentum = 0.8\n",
        "    input_shape = (384, 384, 3)\n",
        "\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Add the first convolution block\n",
        "    dis1 = Conv2D(filters=96, kernel_size=3, strides=1, padding='same')(input_layer)\n",
        "    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n",
        "\n",
        "    # Add the 2nd convolution block\n",
        "    dis2 = Conv2D(filters=96, kernel_size=3, strides=2, padding='same')(dis1)\n",
        "    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n",
        "    dis2 = BatchNormalization(momentum=momentum)(dis2)\n",
        "\n",
        "    # Add the third convolution block\n",
        "    dis3 = Conv2D(filters=192, kernel_size=3, strides=1, padding='same')(dis2)\n",
        "    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n",
        "    dis3 = BatchNormalization(momentum=momentum)(dis3)\n",
        "\n",
        "    # Add the fourth convolution block\n",
        "    dis4 = Conv2D(filters=192, kernel_size=3, strides=2, padding='same')(dis3)\n",
        "    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n",
        "    dis4 = BatchNormalization(momentum=0.8)(dis4)\n",
        "\n",
        "    # Add the fifth convolution block\n",
        "    dis5 = Conv2D(384, kernel_size=3, strides=1, padding='same')(dis4)\n",
        "    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n",
        "    dis5 = BatchNormalization(momentum=momentum)(dis5)\n",
        "\n",
        "    # Add the sixth convolution block\n",
        "    dis6 = Conv2D(filters=384, kernel_size=3, strides=2, padding='same')(dis5)\n",
        "    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n",
        "    dis6 = BatchNormalization(momentum=momentum)(dis6)\n",
        "\n",
        "    # Add the seventh convolution block\n",
        "    dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)\n",
        "    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n",
        "    dis7 = BatchNormalization(momentum=momentum)(dis7)\n",
        "\n",
        "    # Add the eight convolution block\n",
        "    dis8 = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(dis7)\n",
        "    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n",
        "    dis8 = BatchNormalization(momentum=momentum)(dis8)\n",
        "\n",
        "    # Add a dense layer\n",
        "    dis9 = Dense(units=1024)(dis8)\n",
        "    dis9 = LeakyReLU(alpha=0.2)(dis9)\n",
        "\n",
        "    # Last dense layer - for classification\n",
        "    output = Dense(units=1, activation='sigmoid')(dis9)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_vgg():\n",
        "    \"\"\"\n",
        "    Build VGG network to extract image features\n",
        "    \"\"\"\n",
        "    input_shape = (384, 384, 3)\n",
        "\n",
        "    # Load a pre-trained VGG19 model trained on 'Imagenet' dataset\n",
        "    vgg = VGG19(weights=\"imagenet\")\n",
        "    vgg.outputs = [vgg.layers[9].output]\n",
        "\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Extract features\n",
        "    features = vgg(input_layer)\n",
        "\n",
        "    # Create a Keras model\n",
        "    model = Model(inputs=[input_layer], outputs=[features])\n",
        "    return model\n",
        "\n",
        "\n",
        "def sample_images(data_dir, batch_size, high_resolution_shape, low_resolution_shape):\n",
        "    # Make a list of all images inside the data directory\n",
        "    all_images = glob.glob(data_dir)\n",
        "\n",
        "    # Choose a random batch of images\n",
        "    images_batch = np.random.choice(all_images, size=batch_size)\n",
        "\n",
        "    low_resolution_images = []\n",
        "    high_resolution_images = []\n",
        "\n",
        "    for img in images_batch:\n",
        "        # Get an ndarray of the current image\n",
        "        img1 = imread(img, mode='RGB')\n",
        "        img1 = img1.astype(np.float32)\n",
        "\n",
        "        # Resize the image\n",
        "        img1_high_resolution = imresize(img1, high_resolution_shape)\n",
        "        img1_low_resolution = imresize(img1, low_resolution_shape)\n",
        "\n",
        "        # Do a random horizontal flip\n",
        "        if np.random.random() < 0.5:\n",
        "            img1_high_resolution = np.fliplr(img1_high_resolution)\n",
        "            img1_low_resolution = np.fliplr(img1_low_resolution)\n",
        "\n",
        "        high_resolution_images.append(img1_high_resolution)\n",
        "        low_resolution_images.append(img1_low_resolution)\n",
        "\n",
        "    # Convert the lists to Numpy NDArrays\n",
        "    return np.array(high_resolution_images), np.array(low_resolution_images)\n",
        "\n",
        "\n",
        "def save_images(low_resolution_image, original_image, generated_image, path):\n",
        "    \"\"\"\n",
        "    Save low-resolution, high-resolution(original) and\n",
        "    generated high-resolution images in a single image\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 3, 1)\n",
        "    ax.imshow(low_resolution_image)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Low-resolution\")\n",
        "\n",
        "    ax = fig.add_subplot(1, 3, 2)\n",
        "    ax.imshow(original_image)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Original\")\n",
        "\n",
        "    ax = fig.add_subplot(1, 3, 3)\n",
        "    ax.imshow(generated_image)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Generated\")\n",
        "\n",
        "    plt.savefig(path)\n",
        "\n",
        "def PSNR(original, compressed): \n",
        "    mse = np.mean((original - compressed) ** 2) \n",
        "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
        "                  # Therefore PSNR have no importance. \n",
        "        return 100\n",
        "    max_pixel = 255.0\n",
        "    psnr = 20 * log10(max_pixel / sqrt(mse)) \n",
        "    return psnr \n",
        "\n",
        "def write_log(callback, name, value, batch_no):\n",
        "    \"\"\"\n",
        "    Write scalars to Tensorboard\n",
        "    \"\"\"\n",
        "    #summary = tf.Summary()\n",
        "    #summary_value = summary.value.add()\n",
        "    #summary_value.simple_value = value\n",
        "    #summary_value.tag = name\n",
        "    #callback.writer.add_summary(summary, batch_no)\n",
        "    #callback.writer.flush()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_dir = \"/content/drive/My Drive/DIV2K_train_HR/*.png\"\n",
        "    epochs = 30000\n",
        "    batch_size = 1\n",
        "    mode = 'train'\n",
        "    loss1 = []\n",
        "    loss2 = []\n",
        "    psnrv = []\n",
        "\n",
        "    # Shape of low-resolution and high-resolution images\n",
        "    low_resolution_shape = (96, 96, 3)\n",
        "    high_resolution_shape = (384, 384, 3)\n",
        "\n",
        "    # Common optimizer for all networks\n",
        "    common_optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "    if mode == 'train':\n",
        "        # Build and compile VGG19 network to extract features\n",
        "        vgg = build_vgg()\n",
        "        vgg.trainable = False\n",
        "        vgg.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Build and compile the discriminator network\n",
        "        discriminator = build_discriminator()\n",
        "        discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator network\n",
        "        generator = build_generator()\n",
        "\n",
        "        \"\"\"\n",
        "        Build and compile the adversarial model\n",
        "        \"\"\"\n",
        "\n",
        "        # Input layers for high-resolution and low-resolution images\n",
        "        input_high_resolution = Input(shape=high_resolution_shape)\n",
        "        input_low_resolution = Input(shape=low_resolution_shape)\n",
        "\n",
        "        # Generate high-resolution images from low-resolution images\n",
        "        generated_high_resolution_images = generator(input_low_resolution)\n",
        "\n",
        "        # Extract feature maps of the generated images\n",
        "        features = vgg(generated_high_resolution_images)\n",
        "\n",
        "        # Make the discriminator network as non-trainable\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        # Get the probability of generated high-resolution images\n",
        "        probs = discriminator(generated_high_resolution_images)\n",
        "\n",
        "        # Create and compile an adversarial model\n",
        "        adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n",
        "        adversarial_model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n",
        "\n",
        "        # Add Tensorboard\n",
        "        #tensorboard = TensorBoard(log_dir=\"/content/drive/My Drive/SR_GAN/logs/\".format(time.time()))\n",
        "        #tensorboard.set_model(generator)\n",
        "        #tensorboard.set_model(discriminator)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\"Epoch:{}\".format(epoch))\n",
        "\n",
        "            \"\"\"\n",
        "            Train the discriminator network\n",
        "            \"\"\"\n",
        "            print(\"hello1\")\n",
        "            # Sample a batch of images\n",
        "            high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n",
        "                                                                          low_resolution_shape=low_resolution_shape,\n",
        "                                                                          high_resolution_shape=high_resolution_shape)\n",
        "            print(\"hello2\")\n",
        "            # Normalize images\n",
        "            high_resolution_images = high_resolution_images / 127.5 - 1.\n",
        "            low_resolution_images = low_resolution_images / 127.5 - 1.\n",
        "\n",
        "            print(\"hello3\")\n",
        "            # Generate high-resolution images from low-resolution images\n",
        "            generated_high_resolution_images = generator.predict(low_resolution_images)\n",
        "            print(\"hello4\")\n",
        "            # Generate batch of real and fake labels\n",
        "            real_labels = np.ones((batch_size, 16, 16, 1))\n",
        "            fake_labels = np.zeros((batch_size, 16, 16, 1))\n",
        "            print(\"hello5\")\n",
        "            # Train the discriminator network on real and fake images\n",
        "            d_loss_real = discriminator.train_on_batch(high_resolution_images, real_labels)\n",
        "            d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n",
        "            print(\"hello6\")\n",
        "            # Calculate total discriminator loss\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            print(\"d_loss:\", d_loss)\n",
        "\n",
        "            \"\"\"\n",
        "            Train the generator network\n",
        "            \"\"\"\n",
        "\n",
        "            # Sample a batch of images\n",
        "            high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n",
        "                                                                          low_resolution_shape=low_resolution_shape,\n",
        "                                                                          high_resolution_shape=high_resolution_shape)\n",
        "            # Normalize images\n",
        "            high_resolution_images = high_resolution_images / 127.5 - 1.\n",
        "            low_resolution_images = low_resolution_images / 127.5 - 1.\n",
        "\n",
        "            # Extract feature maps for real high-resolution images\n",
        "            image_features = vgg.predict(high_resolution_images)\n",
        "\n",
        "            # Train the generator network\n",
        "            g_loss = adversarial_model.train_on_batch([low_resolution_images, high_resolution_images],\n",
        "                                             [real_labels, image_features])\n",
        "\n",
        "            print(\"g_loss:\", g_loss)\n",
        "\n",
        "            # Write the losses to Tensorboard\n",
        "            #write_log(tensorboard, 'g_loss', g_loss[0], epoch)\n",
        "            #write_log(tensorboard, 'd_loss', d_loss[0], epoch)\n",
        "\n",
        "            # Sample and save images after every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n",
        "                                                                              low_resolution_shape=low_resolution_shape,\n",
        "                                                                              high_resolution_shape=high_resolution_shape)\n",
        "                # Normalize images\n",
        "                high_resolution_images = high_resolution_images / 127.5 - 1.\n",
        "                low_resolution_images = low_resolution_images / 127.5 - 1.\n",
        "\n",
        "                generated_images = generator.predict_on_batch(low_resolution_images)\n",
        "                temp = 0\n",
        "                c = 0\n",
        "                for index, img in enumerate(generated_images):\n",
        "                    c = c + 1\n",
        "                    temp = temp + PSNR(high_resolution_images[index],img)\n",
        "                    #save_images(low_resolution_images[index], high_resolution_images[index], img,\n",
        "                                #path=\"/content/drive/My Drive/SR_GAN/results/img_{}_{}\".format(epoch, index))\n",
        "                psnravg = temp/c\n",
        "                print(\"psnr average :\",psnravg)\n",
        "                psnrv.append(psnravg)\n",
        "\n",
        "        # Save models\n",
        "        generator.save_weights(\"/content/drive/My Drive/SR_GAN/weights/generator_final.h5\")\n",
        "        discriminator.save_weights(\"/content/drive/My Drive/SR_GAN/weights/discriminator_final.h5\")\n",
        "\n",
        "    if mode == 'predict':\n",
        "        # Build and compile the discriminator network\n",
        "        discriminator = build_discriminator()\n",
        "        print(discriminator.summary())\n",
        "        # Build the generator network\n",
        "        generator = build_generator()\n",
        "        print(generator.summary())\n",
        "        # Load models\n",
        "        generator.load_weights(\"/content/drive/My Drive/SR_GAN/weights/generator.h5\")\n",
        "        discriminator.load_weights(\"/content/drive/My Drive/SR_GAN/weights/discriminator.h5\")\n",
        "\n",
        "        # Get 10 random images\n",
        "        high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=10,\n",
        "                                                                      low_resolution_shape=low_resolution_shape,\n",
        "                                                                      high_resolution_shape=high_resolution_shape)\n",
        "        \n",
        "        # Normalize images\n",
        "        high_resolution_images = high_resolution_images / 127.5 - 1.\n",
        "        low_resolution_images = low_resolution_images / 127.5 - 1.\n",
        "\n",
        "        # Generate high-resolution images from low-resolution images\n",
        "        generated_images = generator.predict_on_batch(low_resolution_images)\n",
        "        temp = 0\n",
        "        c = 0\n",
        "        psnrv = []\n",
        "        for index, img in enumerate(generated_images):\n",
        "          c = c + 1\n",
        "          temp = temp + PSNR(high_resolution_images[index],img)\n",
        "          save_images(low_resolution_images[index], high_resolution_images[index], img,path=\"/content/drive/My Drive/SR_GAN/results/img_{}\".format(index))\n",
        "          psnravg = temp/c\n",
        "          print(\"psnr average :\",psnravg)\n",
        "          psnrv.append(psnravg)\n",
        "        \n",
        "        # Save images\n",
        "        #for index, img in enumerate(generated_images):\n",
        "            #save_images(low_resolution_images[index], high_resolution_images[index], img,path=\"/content/drive/My Drive/results2/gen_{}\".format(index))\n",
        "            #save_image(img,path=\"/content/drive/My Drive/results2/gen_{}\".format(index))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}